\documentclass[a4paper,11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[boxed]{algorithm}
\usepackage{algorithmic}
%\usepackage{fancyhdr}
%\usepackage{natbib}
%\usepackage[natbib=true]{biblatex}
%\usepackage{epstopdf}
%\usepackage{float}
%\usepackage{url}
\usepackage[pdftex]{hyperref}
\usepackage{enumerate}
\usepackage{array,ragged2e}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\dx}{\, \mathrm{d}}
\newcommand{\de}{\partial}

\newcolumntype{C}[1]{>{\Centering}m{#1}}

\newtheorem{thm}{Theorem}[section]
\renewcommand{\qedsymbol}{$\lozenge$}


%\hoffset = 0cm
%\voffset = -2cm
%\textheight = 700pt
%\marginparwidth = 0pt
%\textwidth = 450pt
%\oddsidemargin = 0pt
%\evensidemargin = 0pt
%\topmargin = 0pt

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\setlength\parindent{0pt}
\setlength{\parskip}{6pt} %%distanza fra i paragrafi


\begin{document}

\title{Finding primes in Parallel: Erathostenes' Sieve}
\author{Davide Taviani\\
(d.taviani@students.uu.nl)}
\date{\today}

\maketitle

\begin{abstract}
This report will discuss the implementation of the most common sieve to find prime numbers and the performance result on various architectures.
\end{abstract}

\tableofcontents

\pagebreak

\section{Introduction}

In this report we document the use of the Bulk Synchronous Programming paradigm in order to find prime numbers in parallel, using the well known Erathostenes' Sieve.

After giving a sequential algorithm and discussing some of the possibile optimizations, we introduce the algorithm used in parallel and theoretically compute its BSP cost, which takes into account not only computation, but also communication and synchronization.

In addition, after using a benchmarking tool to measure the performances of some machines and gain more insight on their structure, we run our implementation of the parallel algorithm and compare the actual cost with the predicted one.

\section{Erathostenes' Sieve}

Erathostene's Sieve is a ancient algorithm to find all prime numbers up to any given integer $n$; its first appearance is found into the  ``Introduction of Arithmetic'' of the greek mathematician Nicomachus, which dates back to the first century CE.

\subsection{Description of the sequential algorithm}

In its most simple form, the algorithm proceeds as follows:

\begin{algorithm}
\begin{algorithmic}
\REQUIRE $n \in \N$
\ENSURE The list of primes up to $n$.
\STATE
\STATE Create the list of numbers $L=[2,3,4,...,n]$.
\STATE $p=2$.
\WHILE{$p<\sqrt{n}$}
\STATE Remove all the multiples of $p$ from $L$.
\STATE $p$ is the next element of $L$.
\ENDWHILE
\RETURN $L$
\end{algorithmic}
\end{algorithm}

The correctness of such algorithm derives from the the fact that for every composite integer $n \in \N$, all prime factors are $\leq \sqrt{n}$.

However, some optimizations to this algorithm are possible:

\begin{itemize}
\item We could skip the even numbers when creating $L$; this has also the advantage that halves the memory required for storing it in a computer.
\item For each $p$, the crossing could start at $p^2$ since all the numbers of the form $kp$, where $k<p$ have been already cancelled when considering multiples of $k$.
\end{itemize}

To derive the cost of this algorithm, we will use the fact that the probability of an arbitrary integer $x \geq 2$ to be prime is about $\dfrac{1}{\ln x}$.

The total number of cross-out operations for each number $p$ is then

$$ \dfrac{n}{p} -p+1$$

Using the fact that the prime harmonic series asymptotically approaches $\ln \ln n$, the total cost is $O(n \ln \ln n)$. Note that the term $-p+1$ would subtract an irrelevant value with respect to the result.

In our implementation of this algorithm, we use the optimization mentioned before: the array is made by 2 at index 0, followed by only even numbers; in this way, there is a relation between the index $i \geq 1$ and the number stored in $L[i]$, that is $L[i] = 2*i+1$.

In particular, when we consider $L[i]$, we start the crossing out at $i*(L[i]+1)$ and use a step size of exactly $L[i]$. Using this addition of indexes, we save some flops: addition is cheaper than multiplication and we reuse some numbers previously computed.

\subsection{Parallel implementation}

Our parallel implementation of the Erathostenes' Sieve is based, once again, on the fact that every composite integer up to $n$ has prime factors less than or equal $\sqrt{n}$.

The main idea is that, given the input number $n$, we compute in each processor sequentially the primes up to $\sqrt{n}$, and then we divide the reimaining $n-\sqrt{n}$ numbers into $p$ processors using a block distribution, and perform the crossing-out efficiently in parallel.

This initial sequential step is not a big deviation from our original intent of computing primes in parallel: let $\pi(n)$ be the function that ``counts'' the prime up to $n$; with $n=10^8$ we have that with the sequential algorithm we have to compute only $\pi(\sqrt{n}) = \pi(10^4) = 1229$ prime numbers, while the total number of primes is $\pi(10^8) = 5761455$ numbers. Then, in this case, less than the $0.003\%$ of the primes are found sequentially.

A more precise overview of the algorithm for processor $P(s)$ with $0 \leq s \leq p$ is the following:

\begin{algorithm}
\begin{algorithmic}
\REQUIRE $n \in \N$
\ENSURE The total number of primes computed
\end{algorithmic}

\begin{enumerate}
\item[(0)] \begin{algorithmic}[1]
\STATE $q:= \lfloor \sqrt{n} \rfloor$
\STATE compute sequentially the primes up to $q$ and store it in the array \verb|primes| of size $c$.
\STATE compute the number of local elements $m:= \left\lceil \dfrac{n-q}{2p} \right\rceil$
\STATE Initialize the \verb|localList| of numbers of size $m$
\STATE Initialize the index $i=0$
\STATE Initialize the number $j=2ms+q+1$
\IF{$j$ is even}
\STATE Consider the next number $j = j+1$
\ENDIF
\WHILE{$i<m$ \& $j\leq N$}
\STATE Add the element to the \verb|localList|: \verb|localList[i] = j|
\STATE Update the index and the content $i=i+1, j=j+2$.
\ENDWHILE
\STATE set the (possible) remaining \verb|localList| items to 0
\STATE Initialize the counter of local primes $c_2 = 0$.
\STATE Initialize the temporary number $k$
\FOR{$i=1:c-1$}
\STATE $k$ = \verb|primes[c-i]|
\IF{$2m(s+1)+q < k^2$}
\STATE there's nothing to remove, proceed to the next value of $i$
\ENDIF
\FOR{$j=0:m-1$}
\IF{\texttt{localList[j]}=0}
\STATE The element in the localList has already been removed, continue to the next value of $j$.
\ENDIF
\IF{\texttt{localList[j]} is multiple of $k$}
\STATE $j$ is the first index in which a multiple of $k$ occurs. Exit from the cycle on $j$.
\ENDIF
\ENDFOR
\WHILE{$j<m$}
\STATE \verb|localList[j]| = 0
\STATE increment $j$ by exactly $k$.
\ENDWHILE
\ENDFOR
\STATE Count the nonzeros in \verb|localList| and store it in $c_2$
\IF{s==0}
\STATE $c_2 = c$, in order to count also the numbers in \verb|primes|
\ENDIF
\end{algorithmic}
\item[(1)] put $c_2$ in $P(*)$.
\item[(2)]

\begin{algorithmic}
\STATE sum the entries of $c_2$ in \verb|count|
\end{algorithmic}

\end{enumerate}
\end{algorithm}

Some remarks about the first superstep of algorithm (it is indicated the line which we are referring to):

\begin{itemize}
\item[6:] The assignment $j= 2ms+q+1$ is the first one in which each processor does something different; in particular, this is where the block distribution comes at play: we assign to $j$ the value at which each processor should start to populate \verb|localList|.
\item[7:] We deliberately chose to skip the even numbers, and this statement enforces this: in particular, since we are going to populate adding every other number, with this check we are explicitly sure that we don't put in \verb|localList| \emph{only} the even numbers.
\item[19-21:] With this check, which is due to the fact that we are computing prime numbers with many processors and we are using a block distribution, the number of operations performed dramatically decreases, in particular when the number of processors used is quite high: we check if $k^2$, which we remember being the number from which we should start the cross-out operations, is larger than the largest number present in the \verb|localList| for this particular processor. If this is the case, we will just skip this value of $k$ and save many operations.
\item[17-34:] This is the core of the algorithm: the step in which the parallel crossing-out is performed. The whole idea is the following: we consider a element from \verb|primes|, and we look for the first index in \verb|localList| which identifies a multiple of $k$: we then break from the inner loop and exploit the fact that we used a block distribution and each multiple of $k$ is separated by $k$ numbers (actually, since we stored only the even numbers, the multiples really occur every $2k$ numbers, so every $k$ indexes). So, there's no need to check for multiples with a modulo operation (which is pretty expensive) but we jump from one multiple to the other without any waste.

\item[18:] This assignment is really important for our algorithm to take a reasonable amount of time; let us think about the prime $p$: if the array is full (i.e. $p$ is the first number to be checked for multiples) we have to perform at most $p$ modulo operations (as we said before, once every $p$ numbers there is a multiple of $p$, but, since we consider only odd numbers, we have that this happens every $p$ indexes) before detecting the first index where a multiple of $p$ occurs.

Now let us suppose that we perform the cross-out operations order-wise, meaning that 3 is the first number that has multiples removed, then 5 and so on; when it's the turn of $p$ we have that we need more than $p$ modulo operations before detecting any suitable index! This is due to the fact that we have already removed the numbers $3p$, $5p$,... and so the first multiple of $p$ which we stumble upon is $p^2$. This potentially takes a lot of modulo operations, and if we are talking about very large primes, the time spent looking for multiples dramatically increases.

To give a pratical example of such behaviour, let us consider $n=150$ and $p=1$.

$\lfloor \sqrt{n} \rfloor = 12$, so \verb|primes| $= \{2,3,5,7,11\}$ and \verb|localList| $= \{13,15,17,...,149\}$

Let us suppose, as said before, that we start crossing out from 3 (checking for 2 is useless, there are no even numbers in \verb|localList|): the first number which is a multiple of 3 is \verb|localList[1]|=15, meaning that only 2 modulo operations are needed.

The next prime is 5: the first multiple would have been 15 (with again only 2 modulo operations) but we already removed that: now we have to look for 25, which is stored at the index 6 (then 7 modulo operations are needed). When we start looking for multiples of 7, we see that the number of operations required starts to grow: if \verb|localList| had been full, we would have found 21 (at index 4) but now we have to look for 49 which is at index 18.

For 11, the first number would have been 33, at index 10, while now we have to go to 121, which is far ahead in \verb|localList|. And if this happened even with relatively small size value of $n$, we can understand its massive impact for big numbers such as $10^8$ and $10^9$.

To avoid the ``waste'' of such useful information (the first index which refers to a multiple of $p$), a good way is to start crossing out from the biggest prime to the smallest. Also in this case we have little loss of information (thinking about the previous instance, if we already crossed out multiples of 11, 7 and 5 the first multiple of 3 is not 15, nor 21, but 27). This is similar to what happened before, but not that unconvenient: multiples of 3 occur much more frequently than multiples of bigger numbers, so we don't have to perform a large number of additional modulo operations.
\end{itemize}

\subsection{Analysis of the BSP cost}

As already said, our algorithms works substantially in the following way: given a boundary $n \in \N$, each processor

\begin{itemize}
\item computes the primes up to $q := \lfloor \sqrt{n} \lfloor$
\item creates a local array of $\dfrac{n -q}{p}$ numbers
\item performs the crossing-out
\item broadcasts to all the other processors the amount of primes found ($P(0)$ takes into considerations also the primes up to $q$)
\item sums up the counts in order to have final count of primes below $n$.
\end{itemize}

Wee see that the cost of this outline is the following:

\begin{itemize}
\item $\lfloor \sqrt{n} \rfloor \ln \ln \lfloor \sqrt{n} \rfloor$ for the sequential part
\item $\dfrac{n-\lfloor \sqrt{n} \rfloor}{p} \ln \ln \dfrac{n-\lfloor \sqrt{n} \rfloor}{p}$ for the crossing out
\item $p-1$ for communicating the number of primes found locally (one \verb|int| is sent to $p-1$ processors)
\item $p$ for computing the sum
\end{itemize}

\begin{itemize}
\item $\lfloor \sqrt{n} \rfloor \ln \ln \lfloor \sqrt{n} \rfloor$ for the sequential part
\item $\dfrac{n-\lfloor \sqrt{n} \rfloor}{p} \ln \ln \dfrac{n-\lfloor \sqrt{n} \rfloor}{p}$ for the crossing out
\item $p-1$ for communicating the number of primes found locally (one \verb|int| is sent to $p-1$ processors)
\item $p$ for computing the sum
\end{itemize}

The overall BSP cost is then

$$\lfloor \sqrt{n} \rfloor \ln \ln \lfloor \sqrt{n} \rfloor + \dfrac{n-\lfloor \sqrt{n} \rfloor}{p} \ln \ln \dfrac{n-\lfloor \sqrt{n} \rfloor}{p} + p + (p-1)g + 3l$$

\section{Numerical experiments}

In order to better understand the real behaviour of our implementation of the aforementioned algorithm, we performed a series of numerical tests.

The machines used where the following:

A total of two supercomputers,

\begin{itemize}
\item \textbf{Huygens}: The dutch national supercomputer, a clustered SMP (Symmetric Multiprocessing) IBM pSeries 575 system, with a total of 3456 IBM Power6 cores with clock speed of 4.7 GHz, a total performance of 65 Tflop/s, an internal transfer speed of 160 Gbit/s, 15.75 TB of memory and 700 TB storage capacity.
\item \textbf{Abyssos}: The supercomputer of the Hellenic Centre of Marine Research (HCMR) in Greece, a SGI Altix 3700 system, with 128 Intel Itanium 2 cores with clock speed of 1.5 GHz, 128 GB of memory and an advertised speed of 4 Gflop/s.
\end{itemize}

However, since Abyssos had some cores in maintenance, the maximum number of usable cores was 112.

Also, in order to understand the real computational capabilities of the modern common computers, we ran some tests (although, obviously with only a limited number of processors) also on these computers:

\begin{itemize}
\item MacBook Pro: 2 Intel i7 cores with clock speed 2.4 Ghz, 8GB memory
\item MacBook Air: 2 Intel i5 cores with clock speed 1.7 GHz, 4GB memory
\end{itemize}

\subsection{Benchmarking: $r$, $g$ and $l$}

In order to gain more insights about the performance of the computers mentioned above, we ran the benchmarking program embedded within the bundle \verb|BSPedupack|.

The table below shows the resulting value of the computing rate $r$, the communication cost $g$, and  the synchronization cost $l$ (all times are in flop units):

%\renewcommand*{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{|C{2.8cm}|r r r r|}
\hline
\textbf{Machine} & Processors & $r$ (Mflop/s) & $g$ & $l$ \\
\hline
Huygens & 1 & 195.693 & 51.1 & 554.6 \\
& 2 & 195.137 & 55.0 & 1976.0 \\
& 4 & 195.693 & 57.0 & 4106.9 \\
& 8 & 195.623 & 61.9 & 7476.1 \\
& 16 & 195.426 & 59.9 & 15964.7 \\
& 32 & 195.144 & 61.3 & 32712.1 \\
& 64 & 195.481 & 64.3 & 100689.8 \\
& 128 & 195.551 & 135.7 & 209302.8 \\
& 256 & 195.480 & & \\
\hline
Abyssos & 1 & 1462.335 & 199.8 & 8140.4 \\
& 2 & 1480.572 & 413.5 & 23244.1 \\
& 4 & 1480.638 & 267.0 & 42900.9 \\
& 8 & 1478.579 & 359.4 & 140407.6 \\
& 16 & 1477.364 & 475.0 & 322366.5 \\
& 32 & 1478.145 & 674.2 & 786181.7 \\
& 64 & 1480.354 & 1927.5 & 2186074.4 \\
& 112 & 1478.617 & 8124.5 & 6059875.8 \\
\hline
MacBook Air & 1 & 550.637 & 16.5 & 302.1 \\
& 2 & 530.685 & 63.4 & 1992.9 \\
\hline
MacBook Pro & 1 & 522.502 & 19.4 & 190.4 \\
& 2 & 337.831 & 42.4 & 3799.0 \\
& 4 & 337.350 & 284.0 & 56458.1 \\
\hline
\end{tabular} \end{center}

Remarks:

\begin{itemize}
\item \textbf{Huygens}: From the results we can easily tell the structure of this supercomputer: $g$ is more or less the same for $p=1,...,64$ while doubles for $p=128$. This is due to the fact that in this computer we have small nodes of 32 cores (16 dual-core processors). The communication speed is extremely fast inside the node and fast even with adjacent ones, but it is slower when we want to communicate data between nodes that potentially could be far (in a physical sense); this is the case of 128 cores. This structure also reflects on the fact that the cost of $l$ more or less doubles with the doubling of processors but increases significantly when we go outside the node (from 64 cores onwards).

    Interestingly, the same benchmark with 8 nodes of 32 cores (256 in total) fails to return values for $g$ and $l$.

\item \textbf{Abyssos}: One interesting fact is that the actual speed of this is way lower than the advertised one, but we were not much surprised by this. Again we are able to tell the structure of the machine based on these results, since we have that $l$ increases dramatically after we go from $64$ to $112$ (we have already mentioned that 16 out of 128 cores were not reachable): this is due to the fact that this machine is built in four nodes, each one with 32 cores. When we have to work with more than two nodes, we see that the communication cost increases dramatically.

\subsection{$h$-relations and flops}

\subsubsection{\texttt{bsp\_put}}

In the following figures, we show the relationship between the $h$-relations and the flops, measured again with \verb|bspbench|. Here we will report only few of such plots in order to save some space:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/32-put}
\end{center}
\caption{Huygens: $p=32$}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/abyssos-32-put}
\end{center}
\caption{Abyssos: $p=32$}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/mbair-2-put}
\end{center}
\caption{MacBook Air: $p=2$}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/mbpro-2-put}
\end{center}
\caption{MacBook Pro: $p=2$}
\end{figure}

\subsubsection{\texttt{bsp\_get}}

We also measured, for both the supercomputers, the performances when we replace \verb|bsp_put| by \verb|bsp_get|. The result are shown below:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/32-get}
\end{center}
\caption{Huygens: \texttt{bsp\_get} and $p=32$}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/abyssos-32-get}
\end{center}
\caption{Abyssos: \texttt{bsp\_get} and $p=32$}
\end{figure}



\end{itemize}

\subsection{Experimental results}

Since our implementation, from preliminary testing, seems reasonably fast, we run two series of tests: finding primes below $10^8$ and below $10^9$.

In the following tables we represent the machine, the number of processor used $p$, the time taken for the initial sequential step $t_{seq}$, the time taken to compute the result $t$ (we assume that each processor takes exactly the same time to compute the final result, but in reality there is a difference in timing at most of the order of $10^{-4} s$).

On the dutch supercomputer Huygens, we always reserved full nodes: 1 for $p=1,...,32$, 2 for $p=64$, and so on. For the sequential time we used the interactive mode, in order to take the time with \verb|time|; for this reason, it is important to remark that the result for the sequential algorithm may not be completely reliable since the processor used was also probably in charge of some other task, in the meantime. This is the reason we decided to run our parallel program even with $p=1$ and used this value for our Speedup plot (see below).

A similar thing also applies for the other supercomputer, Abyssos.

\subsubsection{Primes up to $10^8$}

With $N=10^8$, we have that $\sqrt{N} = 10000$. For the sequential time we used the UNIX function \verb|time|, while for the parallel program we used the function \verb|bsp_time|.

\begin{center}
\begin{tabular}{|r r r|r r r|}
\hline
\multicolumn{3}{|c|}{Huygens} & \multicolumn{3}{c|}{Abyssos} \\
\hline
$p$ & $t_{seq}$ & $t$ & $p$ & $t_{seq}$ & $t$ \\
\hline
1 & & 3.195 & 1 & & 7.789 \\
1 \footnotemark[1] &  0.000286  & 3.0742 & 1\footnotemark[1] & 0.000085 & 4.3123 \\
2 & 0.000296 & 1.5163 & 2 & 0.000061 & 2.1834 \\
4 & 0.000298 & 0.7253 & 4 & 0.000078 & 1.0743\\
8 & 0.000333 & 0.4309 & 8 & 0.000117 & 0.8701 \\
16 & 0.000380 & 0.2295 & 16 & 0.000058 & 0.3609 \\
32 & 0.000518 & 0.1553 & 32 & 0.000063 & 0.1000\\
64 & 0.000874 & 0.1235 & 64 & 0.000058 & 0.0752\\
128 & 0.000295 & 0.1114 & 112 & 0.000053 & 0.4309\\
256 & 0.000301 & 0.1258 & & &\\
\hline
\multicolumn{3}{|c|}{MacBook Air} & \multicolumn{3}{c|}{MacBook Pro} \\
\hline
1\footnotemark[1] & 0.000178 & 1.4846 & 1\footnotemark[1] & 0.000115 & 1.8095 \\
2 & 0.000137 & 1.0819 & 2 & 0.000151 & 1.3450 \\
4\footnotemark[2] & 0.000241 & 0.9434 & 4\footnotemark[2]& 0.000192 & 1.1931 \\
\hline
\end{tabular}
\end{center}

Notes:

\footnotemark[1]: This is obtained running the parallel implementation with $p=1$, instead of the sequential algorithm.

\footnotemark[2]: 2 out of 4 processor were virtual. This is possible because the program was run with \texttt{mpirun -np 4}.

We now show the speedup plot for $p=1,...,32$ (with $p>32$ we noticed that the speedup plot is very noisy: it is hard to understand the real impact of the increased number of processors) for Huygens and Abyssos:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/sp1e8}
\end{center}
\end{figure}

The following, instead, is the speedup plot for the laptops:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/sp1e8m}
\end{center}
\end{figure}


\subsubsection{Primes up to $10^9$}

With $N=10^9$, we have that $\sqrt{N} \simeq 31622$.

\begin{center}
\begin{tabular}{|r r r|r r r|}
\hline
\multicolumn{3}{|c|}{Huygens} & \multicolumn{3}{c|}{Abyssos} \\
\hline
$p$ & $t_{seq}$ & $t$ & $p$ & $t_{seq}$ & $t$ \\
\hline
1 & & 35.803 & 1 & & 29.383 \\
1\footnotemark[1] & 0.000845  & 35.6322 &  1\footnotemark[1] & 0.000145 & 48.7383  \\
2 & 0.000849 & 17.3429 & 2 & 0.000147 & 24.3291 \\
4 & 0.000864 & 8.6553 & 4 & 0.000151 & 21.1777 \\
8 & 0.000862 & 5.7959 & 8 & 0.000170 &  10.4823\\
16 & 0.000991 & 3.2012 & 16 & 0.000150 & 5.4085\\
32 & 0.001019 & 1.9018 & 32 & 0.000145 & 2.8799\\
64 & 0.000851 & 1.2257 & 64 & 0.000152 & 1.6304\\
128 & 0.000871  & 0.9219 & 112 & 0.000270 & 2.7395\\
256 & 0.000956 & 0.8252 & & & \\
\hline
\multicolumn{3}{|c|}{MacBook Air} & \multicolumn{3}{c|}{MacBook Pro} \\
\hline
1\footnotemark[1] & 0.000393 & 100.5360 & 1\footnotemark[1] & 0.000391  & 23.1023\\
2 & 0.000380 & 14.422739 &  2 &  0.000465 & 16.316857 \\
4\footnotemark[2] & 0.000515 & 12.102203 & 4\footnotemark[2] &  0.000481 & 13.887133 \\
\hline
\end{tabular} \end{center}

Notes:

\footnotemark[1]: This is obtained running the parallel implementation with $p=1$, instead of the sequential algorithm.

\footnotemark[2]: 2 out of 4 processor were virtual. This is possible because the program was run with \texttt{mpirun -np 4}.

We now show the speedup plot for $p=1,...,32$ (not for $p>32$ for the same reasons as above) for Huygens and Abyssos:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/sp1e9}
\end{center}
\end{figure}

The following, instead, is the speedup plot for the laptops:

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{img/sp1e9m}
\end{center}
\end{figure}

\section{Conclusions}

From the previous results, it is evident that a parallel approach for the generation of random numbers using Erathostenes' Sieve is convenient: beside from the occasional noise due to concurrency with other users, we have been able to speed up considerably (think about $n=10^9$ and $p=256$, passing from 35 seconds to less than one second) the generation of primes.

In general there exist a large Even if they don't directly employ Erathostenes' Sieve, there exists This is the reason because there exists a number of ``distributed computing projects''

\end{document} 